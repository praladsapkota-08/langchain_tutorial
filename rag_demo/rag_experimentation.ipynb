{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c021da-d61f-4452-a991-2e71425ed306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyPDF2\n",
    "# !pip install langchain\n",
    "# !pip install --upgrade langchain\n",
    "# !pip install -U langchain-text-splitters\n",
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c8b518-0fe7-4e1e-ba48-72c33f90e821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0724e2e8-745b-4a71-869e-89f53f49c958",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = './data/book.pdf'\n",
    "reader = PdfReader(pdf_path) # it creates object of pdfreader where pdf is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ec45a2-990e-4174-83ea-da9335c287e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_page = reader.pages[30] # .page reads the content \n",
    "print(first_page)\n",
    "text = first_page.extract_text() # .extract_text extract the text from the page and it doesnt read img \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ad8fc6-0312-4907-8547-4d9d551a7ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('first page text:\\n')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4664e0-c702-405e-8986-6b24692a9e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b105b8bc-e479-4824-ba9f-7d92db97d1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = './data/book.pdf'\n",
    "reader = PdfReader(pdf_path)\n",
    "print(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03bd200-56bf-455d-be28-6dbd0ff8644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = []\n",
    "\n",
    "for i, page in enumerate(reader.pages[30:35]):\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        doc = Document(\n",
    "            page_content=text,\n",
    "            metadata = {'page_number' : i + 1, 'source': pdf_path}\n",
    "        )\n",
    "        document.append(doc)\n",
    "\n",
    "print('page sonten preview:\\n',document[2].page_content)\n",
    "print('metadata:\\n',document[2].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41571036-fb9e-43f4-817e-327dbacb6399",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2cc8f7-3fd6-4ae1-8b18-a5f9d428578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 50\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(document)\n",
    "\n",
    "print('total chunks created: \\n', len(chunks))\n",
    "print('first chunk preview: \\n', chunks[2].page_content)\n",
    "print('metadata of first chunk: \\n', chunks[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e693f7-3b86-481c-aa97-8f341b476b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "# Example second splitter\n",
    "token_splitter = TokenTextSplitter(chunk_size=128, chunk_overlap=20)\n",
    "\n",
    "# Let user choose\n",
    "def chunk_documents(documents, strategy=\"recursive\"):\n",
    "    if strategy == \"recursive\":\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    elif strategy == \"token\":\n",
    "        splitter = TokenTextSplitter(chunk_size=128, chunk_overlap=20)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown strategy\")\n",
    "    return splitter.split_documents(documents)\n",
    "You sent\n",
    "from langchain_qdrant import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(\":memory:\")  # or connect to a running Qdrant server\n",
    "vectorstore = Qdrant.from_documents(chunks, embedding_model, client=client, collection_name=\"my_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c1adc75-d6c8-40bc-a775-924fab0c854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b1f17d0-f787-4072-a385-ffdbd0959078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chunk created 469\n",
      "firts chunk preview Physics explores the fundamental forces of nature, including gravity, electromagnetism, and nuclear interactions. It provides the foundation for understanding how the universe behaves at both macroscopic and microscopic levels.\n",
      "\n",
      "Modern literature explores themes of identity, alienation, and social change. Authors use diverse styles and genres to express contemporary concerns.\n",
      "\n",
      "The Renaissance was a period of great cultural and intellectual growth in Europe, marked by advancements in art, science, and philosophy. It laid the groundwork for the modern age.\n",
      "\n",
      "Literature encompasses written works, especially those considered to have artistic or intellectual value. It reflects cultural values, human experiences,\n",
      "second chunk preview  works, especially those considered to have artistic or intellectual value. It reflects cultural values, human experiences, and societal issues.\n",
      "\n",
      "The invention of the internet revolutionized how people access information, communicate, and conduct business. It has become an integral part of modern society.\n",
      "\n",
      "The Renaissance was a period of great cultural and intellectual growth in Europe, marked by advancements in art, science, and philosophy. It laid the groundwork for the modern age.\n",
      "\n",
      "Ancient civilizations such as Egypt, Mesopotamia, and the Indus Valley developed complex societies with writing systems, governance, and architectural achievements.\n",
      "\n",
      "Physics explores the fundamental forces\n"
     ]
    }
   ],
   "source": [
    "with open('../rag_demo/data/educational_corpus.txt', 'r', encoding = 'utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size = 128,\n",
    "    chunk_overlap = 20\n",
    ")\n",
    "\n",
    "chunks = token_splitter.split_text(text)\n",
    "\n",
    "print('total chunk created', len(chunks))\n",
    "print('firts chunk preview', chunks[0])\n",
    "print('second chunk preview', chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54fffe0-fd53-4995-9cb6-a587822d4f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7f0480-ecef-4b5c-ae77-588230d4f9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name =\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "sample_chunks = chunks[:3]\n",
    "embeddings = embedding_model.embed_documents([chunk.page_content for chunk in sample_chunks])\n",
    "\n",
    "print(\"Number of embeddings created:\", len(embeddings)) \n",
    "print(\"Length of one embedding vector:\", len(embeddings[0])) \n",
    "print(\"First 10 values of first embedding:\\n\", embeddings[0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb83f75-c54d-4a31-b3c7-7b278c32be7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3459bed7-bf2a-4931-a8c3-aa3d3bde9b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name =\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
    "\n",
    "vectorstore.save_local('faiss_index')\n",
    "\n",
    "print('FAISS index created and saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59d714f-87f7-4b04-a548-d374ec17353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'why machine learning is used'\n",
    "query1 = 'Why Use Machine Learning?'\n",
    "\n",
    "results = vectorstore.similarity_search(query, k =3)\n",
    "results1 = vectorstore.similarity_search(query1, k =3)\n",
    "\n",
    "for i, res in enumerate(results):\n",
    "    print(f'\\n-------- Results {i + 1} -------')\n",
    "    print('page content preview: \\n', res.page_content[:300])\n",
    "    print('metadata: \\n', res.metadata)\n",
    "\n",
    "print('\\n\\n\\n')\n",
    "\n",
    "for i, res in enumerate(results1):\n",
    "    print(f'\\n-------- Results {i + 1} -------')\n",
    "    print('page content preview: \\n', res.page_content[:300])\n",
    "    print('metadata: \\n', res.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614ca149-8269-4a79-8f9e-b98521826444",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5950216b-d346-4c10-b164-4ee2eefae702",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pipeline = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db50720-bb31-4131-b0e9-2348b9c24781",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\" You are a helpful assistant. \n",
    "Answer the question strictly using the provided context. \n",
    "If the answer is not in the context, say \"I don't know.\" \n",
    "\n",
    "Context: \n",
    "{context} \n",
    "\n",
    "Question: \n",
    "{question} \n",
    "\n",
    "Answer: \n",
    "\"\"\"\n",
    "\n",
    "query = 'Why Use Machine Learning?'\n",
    "\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "context = '\\n\\n'.join([res.page_content for res in results])\n",
    "\n",
    "prompt = prompt_template.format(context=context, question=query)\n",
    "\n",
    "answer = qa_pipeline(prompt, max_length = 200)[0]['generated_text']\n",
    "\n",
    "print('final answer:\\n', answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "005fdf77-98db-4215-b18f-fed467c5e504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Answer:\n",
      " â€¢Getting insights about complex problems and large amounts of data\n",
      "\n",
      "Retrieved Chunks Metadata:\n",
      "{'page_number': 33, 'source': './data/book.pdf'}\n",
      "{'page_number': 31, 'source': './data/book.pdf'}\n",
      "{'page_number': 33, 'source': './data/book.pdf'}\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from transformers import pipeline\n",
    "\n",
    "# -----------------------------\n",
    "# Define the RAG pipeline\n",
    "# -----------------------------\n",
    "def rag_pipeline(pdf_path: str, query: str, start_page: int = 0, end_page: int = 5):\n",
    "    # Step 1: Read PDF\n",
    "    reader = PdfReader(pdf_path)\n",
    "    documents = []\n",
    "    for i, page in enumerate(reader.pages[start_page:end_page]):\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            doc = Document(\n",
    "                page_content=text,\n",
    "                metadata={\"page_number\": i + start_page + 1, \"source\": pdf_path}\n",
    "            )\n",
    "            documents.append(doc)\n",
    "\n",
    "    # Step 2: Split into chunks\n",
    "    character_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    character_chunks = character_splitter.split_documents(documents)\n",
    "\n",
    "    # Step 3: Create embeddings\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.from_documents(character_chunks, embedding_model)\n",
    "\n",
    "    # Step 4: Similarity search\n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "    context = \"\\n\\n\".join([res.page_content for res in results])\n",
    "\n",
    "    # Step 5: Strict prompt\n",
    "    prompt_template = \"\"\"You are a helpful assistant.\n",
    "Answer the question strictly using the provided context.\n",
    "If the answer is not in the context, say \"I don't know.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    prompt = prompt_template.format(context=context, question=query)\n",
    "\n",
    "    # Step 6: Generation with Flan-T5\n",
    "    qa_pipeline = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "    answer = qa_pipeline(prompt, max_length=200)[0][\"generated_text\"]\n",
    "\n",
    "    return answer, results\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage\n",
    "# -----------------------------\n",
    "pdf_path = \"./data/book.pdf\"\n",
    "query = \"Why Use Machine Learning?\"\n",
    "\n",
    "final_answer, retrieved_chunks = rag_pipeline(pdf_path, query, start_page=30, end_page=35)\n",
    "\n",
    "print(\"\\nFinal Answer:\\n\", final_answer)\n",
    "print(\"\\nRetrieved Chunks Metadata:\")\n",
    "for res in retrieved_chunks:\n",
    "    print(res.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20e53aa-4abb-41e5-b6b0-d897f3f46b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from transformers import pipeline\n",
    "\n",
    "# -----------------------------\n",
    "# Define the RAG pipeline\n",
    "# -----------------------------\n",
    "def rag_pipeline(pdf_path: str, query: str, start_page: int = 0, end_page: int = 5):\n",
    "    print(\"Step 1: Reading PDF pages...\")\n",
    "    reader = PdfReader(pdf_path)\n",
    "    documents = []\n",
    "    for i, page in enumerate(reader.pages[start_page:end_page]):\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            doc = Document(\n",
    "                page_content=text,\n",
    "                metadata={\"page_number\": i + start_page + 1, \"source\": pdf_path}\n",
    "            )\n",
    "            documents.append(doc)\n",
    "    print(f\"âœ… Extracted {len(documents)} pages of text from PDF.\")\n",
    "\n",
    "    print(\"\\nStep 2: Splitting documents into chunks...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = splitter.split_documents(documents)\n",
    "    print(f\"âœ… Created {len(chunks)} text chunks.\")\n",
    "\n",
    "    print(\"\\nStep 3: Creating embeddings...\")\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
    "    print(\"âœ… Embeddings generated and stored in FAISS vector database.\")\n",
    "\n",
    "    print(\"\\nStep 4: Performing similarity search...\")\n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "    print(f\"âœ… Retrieved {len(results)} most relevant chunks for the query.\")\n",
    "\n",
    "    context = \"\\n\\n\".join([res.page_content for res in results])\n",
    "\n",
    "    print(\"\\nStep 5: Building strict prompt...\")\n",
    "    prompt_template = \"\"\"You are a helpful assistant.\n",
    "Answer the question strictly using the provided context.\n",
    "If the answer is not in the context, say \"I don't know.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    prompt = prompt_template.format(context=context, question=query)\n",
    "    print(\"âœ… Prompt prepared for generation.\")\n",
    "\n",
    "    print(\"\\nStep 6: Generating answer with Flan-T5...\")\n",
    "    qa_pipeline = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "    answer = qa_pipeline(prompt, max_length=200)[0][\"generated_text\"]\n",
    "    print(\"âœ… Answer generated successfully.\")\n",
    "\n",
    "    return answer, results\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage\n",
    "# -----------------------------\n",
    "pdf_path = \"./data/book.pdf\"\n",
    "query = \"Why Use Machine Learning?\"\n",
    "\n",
    "print(\"\\nðŸš€ Starting RAG pipeline...\\n\")\n",
    "final_answer, retrieved_chunks = rag_pipeline(pdf_path, query, start_page=30, end_page=35)\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"Final Answer:\\n\", final_answer)\n",
    "print(\"==============================\")\n",
    "print(\"\\nRetrieved Chunks Metadata:\")\n",
    "for res in retrieved_chunks:\n",
    "    print(res.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dd4c08-d3eb-47f4-87e2-a92648f9dc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from transformers import pipeline\n",
    "\n",
    "# -----------------------------\n",
    "# Define the RAG pipeline for TXT\n",
    "# -----------------------------\n",
    "def rag_pipeline_txt(txt_path: str, query: str):\n",
    "    print(\"Step 1: Reading TXT file...\")\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    print(f\"âœ… Loaded text file: {txt_path} (length: {len(text)} characters)\")\n",
    "\n",
    "    print(\"\\nStep 2: Wrapping into a Document...\")\n",
    "    documents = [Document(page_content=text, metadata={\"source\": txt_path})]\n",
    "    print(f\"âœ… Created {len(documents)} Document object.\")\n",
    "\n",
    "    print(\"\\nStep 3: Splitting into chunks...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = splitter.split_documents(documents)\n",
    "    print(f\"âœ… Split into {len(chunks)} chunks.\")\n",
    "\n",
    "    print(\"\\nStep 4: Creating embeddings...\")\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
    "    print(\"âœ… Embeddings generated and stored in FAISS vector database.\")\n",
    "\n",
    "    print(\"\\nStep 5: Performing similarity search...\")\n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "    print(f\"âœ… Retrieved {len(results)} most relevant chunks for the query.\")\n",
    "\n",
    "    context = \"\\n\\n\".join([res.page_content for res in results])\n",
    "\n",
    "    print(\"\\nStep 6: Building strict prompt...\")\n",
    "    prompt_template = \"\"\"You are a helpful assistant.\n",
    "Answer the question strictly using the provided context.\n",
    "If the answer is not in the context, say \"I don't know.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    prompt = prompt_template.format(context=context, question=query)\n",
    "    print(\"âœ… Prompt prepared for generation.\")\n",
    "\n",
    "    print(\"\\nStep 7: Generating answer with Flan-T5...\")\n",
    "    qa_pipeline = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "    answer = qa_pipeline(prompt, max_length=200)[0][\"generated_text\"]\n",
    "    print(\"âœ… Answer generated successfully.\")\n",
    "\n",
    "    return answer, results\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage\n",
    "# -----------------------------\n",
    "txt_path = \"./data/educational_corpus.txt\"\n",
    "query = \"Explain the Renaissance.\"\n",
    "\n",
    "print(\"\\nðŸš€ Starting RAG pipeline for TXT...\\n\")\n",
    "final_answer, retrieved_chunks = rag_pipeline_txt(txt_path, query)\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"Final Answer:\\n\", final_answer)\n",
    "print(\"==============================\")\n",
    "print(\"\\nRetrieved Chunks Metadata:\")\n",
    "for res in retrieved_chunks:\n",
    "    print(res.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43f3f717-9dad-425d-aaa7-4de20344fe42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Starting RAG pipeline for TXT...\n",
      "\n",
      "Step 1: Reading TXT file...\n",
      "âœ… Loaded text file: ./data/ott_subscription_faq.txt (length: 2904 characters)\n",
      "\n",
      "Step 2: Wrapping into a Document...\n",
      "âœ… Created 1 Document object.\n",
      "\n",
      "Step 3: Splitting into chunks...\n",
      "âœ… Split into 5 chunks.\n",
      "\n",
      "Step 4: Creating embeddings...\n",
      "âœ… Embeddings generated and stored in FAISS vector database.\n",
      "\n",
      "Step 5: Performing similarity search...\n",
      "âœ… Retrieved 3 most relevant chunks for the query.\n",
      "\n",
      "Step 6: Building strict prompt...\n",
      "âœ… Prompt prepared for generation.\n",
      "\n",
      "Step 7: Generating answer with Flan-T5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (731 > 512). Running this sequence through the model will result in indexing errors\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Answer generated successfully.\n",
      "\n",
      "==============================\n",
      "Final Answer:\n",
      " 3500 for 12 months of Netflix (1 screen).\n",
      "==============================\n",
      "\n",
      "Retrieved Chunks Metadata:\n",
      "{'source': './data/ott_subscription_faq.txt'}\n",
      "{'source': './data/ott_subscription_faq.txt'}\n",
      "{'source': './data/ott_subscription_faq.txt'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from transformers import pipeline\n",
    "\n",
    "# -----------------------------\n",
    "# Define the RAG pipeline for TXT\n",
    "# -----------------------------\n",
    "def rag_pipeline_txt(txt_path: str, query: str):\n",
    "    print(\"Step 1: Reading TXT file...\")\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    print(f\"âœ… Loaded text file: {txt_path} (length: {len(text)} characters)\")\n",
    "\n",
    "    print(\"\\nStep 2: Wrapping into a Document...\")\n",
    "    documents = [Document(page_content=text, metadata={\"source\": txt_path})]\n",
    "    print(f\"âœ… Created {len(documents)} Document object.\")\n",
    "\n",
    "    print(\"\\nStep 3: Splitting into chunks...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=750, chunk_overlap=75)\n",
    "    chunks = splitter.split_documents(documents)\n",
    "    print(f\"âœ… Split into {len(chunks)} chunks.\")\n",
    "\n",
    "    print(\"\\nStep 4: Creating embeddings...\")\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
    "    print(\"âœ… Embeddings generated and stored in FAISS vector database.\")\n",
    "\n",
    "    print(\"\\nStep 5: Performing similarity search...\")\n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "    # for i, res in enumerate(results):\n",
    "    #     print(f'\\n-------- Results {i + 1} -------')\n",
    "    #     print('page content preview: \\n', res.page_content[:300])\n",
    "    #     print('metadata: \\n', res.metadata)\n",
    "\n",
    "    print(f\"âœ… Retrieved {len(results)} most relevant chunks for the query.\")\n",
    "\n",
    "    context = \"\\n\\n\".join([res.page_content for res in results])\n",
    "\n",
    "    print(\"\\nStep 6: Building strict prompt...\")\n",
    "    prompt_template = \"\"\"You are a helpful assistant.\n",
    "Answer the question strictly using the provided context.\n",
    "If the answer is not in the context, say \"I don't know.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    prompt = prompt_template.format(context=context, question=query)\n",
    "    print(\"âœ… Prompt prepared for generation.\")\n",
    "\n",
    "    print(\"\\nStep 7: Generating answer with Flan-T5...\")\n",
    "    qa_pipeline = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "    answer = qa_pipeline(prompt, max_length=200)[0][\"generated_text\"]\n",
    "    print(\"âœ… Answer generated successfully.\")\n",
    "\n",
    "    return answer, results\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage\n",
    "# -----------------------------\n",
    "txt_path = \"./data/ott_subscription_faq.txt\"\n",
    "query = \"Netflix price\"\n",
    "\n",
    "print(\"\\nðŸš€ Starting RAG pipeline for TXT...\\n\")\n",
    "final_answer, retrieved_chunks = rag_pipeline_txt(txt_path, query)\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"Final Answer:\\n\", final_answer)\n",
    "print(\"==============================\")\n",
    "print(\"\\nRetrieved Chunks Metadata:\")\n",
    "for res in retrieved_chunks:\n",
    "    print(res.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82a7e1de-7019-427c-a423-6d719722094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, TokenTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from transformers import pipeline\n",
    "\n",
    "# -----------------------------\n",
    "# Define the RAG pipeline for TXT\n",
    "# -----------------------------\n",
    "def rag_pipeline_txt(txt_path: str, query: str):\n",
    "    print(\"Step 1: Reading TXT file...\")\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    print(f\"âœ… Loaded text file: {txt_path} (length: {len(text)} characters)\")\n",
    "\n",
    "    print(\"\\nStep 2: Wrapping into a Document...\")\n",
    "    documents = [Document(page_content=text, metadata={\"source\": txt_path})]\n",
    "    print(f\"âœ… Created {len(documents)} Document object.\")\n",
    "\n",
    "    # print(\"\\nStep 3: Splitting into chunks...\")\n",
    "    # splitter = RecursiveCharacterTextSplitter(chunk_size=750, chunk_overlap=75)\n",
    "    # chunks = splitter.split_documents(documents)\n",
    "    # print(f\"âœ… Split into {len(chunks)} chunks.\")\n",
    "\n",
    "    print(\"\\nStep 3: Splitting into chunks...\")\n",
    "    splitter = TokenTextSplitter(chunk_size=25, chunk_overlap=5)\n",
    "    chunks = splitter.split_documents(documents)\n",
    "    print(f\"âœ… Split into {len(chunks)} chunks.\")\n",
    "\n",
    "    print(\"\\nStep 4: Creating embeddings...\")\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-mpnet-base-v2\")\n",
    "    vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
    "    print(\"âœ… Embeddings generated and stored in FAISS vector database.\")\n",
    "\n",
    "    print(\"\\nStep 5: Performing similarity search...\")\n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "    # for i, res in enumerate(results):\n",
    "    #     print(f'\\n-------- Results {i + 1} -------')\n",
    "    #     print('page content preview: \\n', res.page_content[:300])\n",
    "    #     print('metadata: \\n', res.metadata)\n",
    "\n",
    "    print(f\"âœ… Retrieved {len(results)} most relevant chunks for the query.\")\n",
    "\n",
    "    context = \"\\n\\n\".join([res.page_content for res in results])\n",
    "\n",
    "    print(\"\\nStep 6: Building strict prompt...\")\n",
    "    prompt_template = \"\"\"You are a helpful assistant.\n",
    "Answer the question strictly using the provided context.\n",
    "If the answer is not in the context, say \"I don't know.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    prompt = prompt_template.format(context=context, question=query)\n",
    "    print(\"âœ… Prompt prepared for generation.\")\n",
    "\n",
    "    print(\"\\nStep 7: Generating answer with Flan-T5...\")\n",
    "    qa_pipeline = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "    answer = qa_pipeline(prompt, max_length=200)[0][\"generated_text\"]\n",
    "    print(\"âœ… Answer generated successfully.\")\n",
    "\n",
    "    return answer, results\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage\n",
    "# -----------------------------\n",
    "# txt_path = \"./data/ott_subscription_faq.txt\"\n",
    "# query = \"1 moonth Netflix\"\n",
    "\n",
    "# print(\"\\nðŸš€ Starting RAG pipeline for TXT...\\n\")\n",
    "# final_answer, retrieved_chunks = rag_pipeline_txt(txt_path, query)\n",
    "\n",
    "# print(\"\\n==============================\")\n",
    "# print(\"Final Answer:\\n\", final_answer)\n",
    "# print(\"==============================\")\n",
    "# print(\"\\nRetrieved Chunks Metadata:\")\n",
    "# for res in retrieved_chunks:\n",
    "#     print(res.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0ff6fe4-cd1c-4ff9-8f8b-472badf5ab78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "please give your query How much does Netflix cost for 3 months?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Starting RAG pipeline for TXT...\n",
      "\n",
      "Step 1: Reading TXT file...\n",
      "âœ… Loaded text file: ./data/ott_subscription_faq.txt (length: 2904 characters)\n",
      "\n",
      "Step 2: Wrapping into a Document...\n",
      "âœ… Created 1 Document object.\n",
      "\n",
      "Step 3: Splitting into chunks...\n",
      "âœ… Split into 46 chunks.\n",
      "\n",
      "Step 4: Creating embeddings...\n",
      "âœ… Embeddings generated and stored in FAISS vector database.\n",
      "\n",
      "Step 5: Performing similarity search...\n",
      "âœ… Retrieved 3 most relevant chunks for the query.\n",
      "\n",
      "Step 6: Building strict prompt...\n",
      "âœ… Prompt prepared for generation.\n",
      "\n",
      "Step 7: Generating answer with Flan-T5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Answer generated successfully.\n",
      "\n",
      "==============================\n",
      "Final Answer:\n",
      " 3500 for 12 months of Netflix (1 screen).\n",
      "==============================\n",
      "\n",
      "Retrieved Chunks Metadata:\n",
      "{'source': './data/ott_subscription_faq.txt'}\n",
      "{'source': './data/ott_subscription_faq.txt'}\n",
      "{'source': './data/ott_subscription_faq.txt'}\n"
     ]
    }
   ],
   "source": [
    "txt_path = \"./data/ott_subscription_faq.txt\"\n",
    "query = input(\"please give your query\")\n",
    "\n",
    "print(\"\\nðŸš€ Starting RAG pipeline for TXT...\\n\")\n",
    "final_answer, retrieved_chunks = rag_pipeline_txt(txt_path, query)\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"Final Answer:\\n\", final_answer)\n",
    "print(\"==============================\")\n",
    "print(\"\\nRetrieved Chunks Metadata:\")\n",
    "for res in retrieved_chunks:\n",
    "    print(res.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb06f0a-c02c-47a9-bc59-d462b450ff65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997fdcfe-0a8a-4c15-94bb-6a6e0428ec6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
